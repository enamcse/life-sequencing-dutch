seed: 42
## TASK AND LOGS
training_task: finetune-prefer
experiment_name: fp-medium-2020
experiment_version: 1 

## OPTIMIZER
optimizer_type: "radam"
learning_rate: 0.01
weight_decay: 0.001
weight_decay_dc: 0.01
beta1: 0.9
beta2: 0.999
layer_lr_decay: 0.95 # for encoder layers
lr_gamma: 0.8
epsilon: 1.e-6

#### CLS SPECIFIC
freeze_embeddings: True
freeze_positions: False
num_targets: 2
loss_type: "entropy"
loss_weights: [0.176, 1]
# pos_weight: 0.5
# asym_penalty: ${asym_penalty}
asym_alpha: 0.025
asym_beta: 1.0

pooled: True
# num_pooled_sep: ${datamodule.task.num_pooled_sep}
pretrained_model_path: ?

stage: finetune
implementation: sth
version: 1