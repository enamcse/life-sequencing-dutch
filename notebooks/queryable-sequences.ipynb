{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explorations to use a database to the people data that we currently have in json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this will provide**\n",
    "\n",
    "The ability to query data that we currently store in json offers to following benefits\n",
    "- can shuffle the person sequence data with only loading the identifier into memory\n",
    "- can easily select certain sequences for further processing, for instance for non-MLM encoding for inference on selected subsamples \n",
    "- can extend the database with\n",
    "    - tabular information from raw data \n",
    "    - store embedding data as lists or vectors (TBD)\n",
    "- process chunks of records in parallel, either through python multiprocessing or through slurm parallelization\n",
    "    - how to re-combine them into one hdf5 file?\n",
    "    - see [here](https://duckdb.org/docs/guides/python/multiple_threads) for some docs on multi-threaded reading and writing with duckb\n",
    "    - how does this work with sqlite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open questions**\n",
    "- this should also work with DuckDB, but we should do some performance tests\n",
    "- we can store embeddings in different tables, also as jsons\n",
    "- for training, we can still store the data in hdf5\n",
    "- how can we fast write during inference? is this fast with sqlite? \n",
    "- create some benchmarks?\n",
    "    - benchmark on speed? \n",
    "    - on parallel processing -> but what do we want to parallelize?\n",
    "    - on reads for downstream tasks\n",
    "    - on writes for inference\n",
    "- time complexity\n",
    "    - there is additional overhead from converting columns to jsons, which is not the case in our current approach \n",
    "    - the questions is whether the gain from being able to query the data easily will outweigh the costs\n",
    "- other considerations \n",
    "    - persistent storage? - not really. sqlite is super stable. more an issue for DuckDB. but how much do we care about really long storage?\n",
    "    - ability to both use all compute available and give results to simple queries on small computers (OSSC work env)\n",
    "- with this approach, can we also query the dicts from the database? ie, `json_extract` \n",
    "- how about writing speed duckdb vs sqlite? ie, when creating a new table, such as the one with the person records?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources**\n",
    "- Claude.ai\n",
    "- https://www.beekeeperstudio.io/blog/sqlite-json-with-text\n",
    "- https://www.sqlite.org/json1.html\n",
    "- https://berthub.eu/articles/posts/big-data-storage/\n",
    "- https://blog.brunk.io/posts/similarity-search-with-duckdb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"person_id\": 1, \n",
    "        \"segment\": [2, 4, 6],\n",
    "        \"age\": [5., 6., 8., 8., 9., 11.],\n",
    "        \"background\": {\"birth_year\": \"year_99.0\", \"birth_month\": \"month_12.0\", \"gender\": \"gender_2.0\", \"origin\": \"municipality_54.0\"},\n",
    "        \"sentence\": [ [\"educSim_2.0\"], [\"_4_D\"], [\"contractType2_1.0\", \"sicknessInsurance2_1.0\", \"wage_50.0\"] ]\n",
    "    },\n",
    "    {\n",
    "        \"person_id\": 2, \n",
    "        \"segment\": [6, 3, 10],\n",
    "        \"age\": [10., 1., 5., 3., 4., 16.],\n",
    "        \"background\": {\"birth_year\": \"year_95.0\", \"birth_month\": \"month_5.0\", \"gender\": \"gender_1.0\", \"origin\": \"municipality_15.0\"},\n",
    "        \"sentence\": [ [\"educSim_1.0\"], [\"contractType2_1.0\", \"_4_D\"], [\"contractType2_1.0\", \"wage_50.0\", \"sicknessInsurance2_2.0\"] ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_person_data(person, columns, json_cols):\n",
    "    output = []\n",
    "    for col in columns:\n",
    "        x = person[col]\n",
    "        if col in json_cols:\n",
    "            x = json.dumps(x)\n",
    "        output.append(x)\n",
    "    return tuple(output)\n",
    "\n",
    "\n",
    "def insert_persons(db_conn, persons, columns, json_cols):\n",
    "    prepared_data = [prepare_person_data(person, columns, json_cols) for person in persons]\n",
    "    db_conn.executemany(\"INSERT INTO persons (person_id, segment, age, background, sentence) VALUES (?, ?, ?, ?, ?)\", prepared_data)\n",
    "    db_conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"mydb.sqlite\"\n",
    "db_conn = sqlite3.connect(dbname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7fdd38199dc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = \"persons\"\n",
    "db_conn.execute(f\"drop table if exists {table_name}\")\n",
    "db_conn.execute(\"\"\"CREATE TABLE \"\"\" + table_name +\n",
    "    \"\"\" (person_id INT NOT NULL\n",
    "        , segment TEXT NOT NULL\n",
    "        , age TEXT NOT NULL\n",
    "        , background TEXT NOT NULL\n",
    "        , sentence TEXT NOT NULL\n",
    "        , PRIMARY KEY ( person_id )\n",
    "        )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vars = [\"segment\", \"age\", \"background\", \"sentence\"]\n",
    "db_cols = [\"person_id\", \"segment\", \"age\", \"background\", \"sentence\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_persons(db_conn, data, columns=db_cols, json_cols=text_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
